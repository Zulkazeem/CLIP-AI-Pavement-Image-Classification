{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zulkazeem/CLIP-AI-Pavement-Image-Classification/blob/main/Using_Roboflow_and_CLIP_for_Pavement_Surface_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOF3Feb7jrnu"
      },
      "source": [
        "# Download and Install CLIP Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyUIWjzOi23X",
        "outputId": "9b26bb4e-04a2-4871-d600-f90689b4da24"
      },
      "source": [
        "#installing some dependencies, CLIP was release in PyTorch\n",
        "import subprocess\n",
        "!pip install torch\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqN0UVpssA7J",
        "outputId": "3098922c-d340-44c8-f7ba-76a76ffd4969"
      },
      "source": [
        "#clone the CLIP repository\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "%cd CLIP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 164 (delta 31), reused 49 (delta 19), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (164/164), 8.87 MiB | 27.51 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/CLIP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWy5SOmV5tLK",
        "outputId": "ba7fb3ff-5f61-4208-a958-0440803c2bd3"
      },
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"xvSbc97nneJxnVuWwVx7\")\n",
        "project = rf.workspace(\"pavement-project\").project(\"pavement-project\")\n",
        "dataset = project.version(1).download(\"clip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.12)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.17.1)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.45.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Requirement already satisfied: scipy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.4)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Pavement-Project-1 to clip:: 100%|██████████| 46349/46349 [00:01<00:00, 26795.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Pavement-Project-1 in clip:: 100%|██████████| 958/958 [00:00<00:00, 5352.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "sFsApb_r65Us",
        "outputId": "ec8a4e96-1085-4515-c8d1-8d48bf7c2313"
      },
      "source": [
        "dataset.location"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Pavement-Project-1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m7FYTSjCN46",
        "outputId": "4efe5cc0-c2ef-401e-8eac-b0c46c6279fb"
      },
      "source": [
        "import os\n",
        "#our the classes and images we want to test are stored in folders in the test set\n",
        "class_names = os.listdir(dataset.location + '/test/')\n",
        "class_names.remove('_tokenization.txt')\n",
        "class_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cracked Non-Cracked',\n",
              " 'Spalling',\n",
              " 'Cracked',\n",
              " 'Joint Non-Cracked',\n",
              " 'Non-Cracked']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QELzUB7pnr-h",
        "outputId": "cda1dcd5-a2b3-4ba7-f4c1-c57b1924a0f7"
      },
      "source": [
        "#we auto generate some example tokenizations in Roboflow but you should edit this file to try out your own prompts\n",
        "#CLIP gets a lot better with the right prompting!\n",
        "#be sure the tokenizations are in the same order as your class_names above!\n",
        "%cat {dataset.location}/test/_tokenization.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example picture from the Pavement Project dataset depicting a Cracked\n",
            "An example picture from the Pavement Project dataset depicting a Cracked, and a Non Cracked\n",
            "An example picture from the Pavement Project dataset depicting a Joint, and a Non Cracked\n",
            "An example picture from the Pavement Project dataset depicting a Non Cracked\n",
            "An example picture from the Pavement Project dataset depicting a Spalling"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPQxIGxXn8bR",
        "outputId": "9fb46cc1-f99b-4a2f-b928-2fae0ac6dae4"
      },
      "source": [
        "#edit your prompts as you see fit here, be sure the classes are in teh same order as above\n",
        "%%writefile {dataset.location}/test/_tokenization.txt\n",
        "An example picture from the Pavement Project dataset depicting a Cracked\n",
        "An example picture from the Pavement Project dataset depicting a Non Cracked\n",
        "An example picture from the Pavement Project dataset depicting a Spalling"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/Pavement-Project-1/test/_tokenization.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXaqPH3oSyO"
      },
      "source": [
        "candidate_captions = []\n",
        "with open(dataset.location + '/test/_tokenization.txt') as f:\n",
        "    candidate_captions = f.read().splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMZJy1SduxiE"
      },
      "source": [
        "# Run CLIP inference on your classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip"
      ],
      "metadata": {
        "id": "vWTB0wTgy5n1",
        "outputId": "db1f2ecf-c412-4d0d-d292-3f018437a41e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clip\n",
            "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6989 sha256=d828eb39fd1f3e21615f3b803979c61ca2ee8179855fce8d40ed41de9602118f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/5c/e6/2c0fdb453a3569188864b17e9676bea8b3b7e160c037117869\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import clip\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "def argmax(iterable):\n",
        "    return max(enumerate(iterable), key=lambda x: x[1])[0]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "correct = []"
      ],
      "metadata": {
        "id": "2Xo7J53N1xfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define our target classificaitons, you can should experiment with these strings of text as you see fit, though, make sure they are in the same order as your class names above\n",
        "#text = clip.tokenize(candidate_captions).to(device)\n",
        "text = processor(candidate_captions, return_tensors=\"pt\", padding=True, truncation=True).to(device)"
      ],
      "metadata": {
        "id": "cGTdRUvP2fKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision\n",
        "!pip install transformers --upgrade"
      ],
      "metadata": {
        "id": "7w2kbgLh3Sm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Adjust size based on CLIP model requirements\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ],
      "metadata": {
        "id": "xO3viwAd3Nmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jR_deQWu80BG",
        "outputId": "33a9fafe-9527-493e-fb41-e96ce18a4af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\Downloads\\pavement_1\test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAi7cvucnFPr"
      },
      "source": [
        "for cls in class_names:\n",
        "    class_correct = []\n",
        "    test_imgs = glob.glob(dataset.location + '/test/' + cls + '/*.jpg')\n",
        "    for img in test_imgs:\n",
        "        #print(img)\n",
        "        image = transform(Image.open(img)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            inputs = processor(text, images=Image.open( test_imgs), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            image_features = model.encode_image(image)\n",
        "            text_features = model.encode_text(text)\n",
        "\n",
        "            logits_per_image, logits_per_text = model(image, text)\n",
        "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "            pred = class_names[argmax(list(probs)[0])]\n",
        "            #print(pred)\n",
        "            if pred == cls:\n",
        "                correct.append(1)\n",
        "                class_correct.append(1)\n",
        "            else:\n",
        "                correct.append(0)\n",
        "                class_correct.append(0)\n",
        "\n",
        "    print('accuracy on class ' + cls + ' is :' + str(sum(class_correct)/len(class_correct)))\n",
        "print('accuracy on all is : ' + str(sum(correct)/len(correct)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import glob\n",
        "\n",
        "# Assuming you have already loaded the CLIP model and processor\n",
        "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "\n",
        "# Assuming class_names is a list of class names in your dataset\n",
        "\n",
        "# Define the data transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "correct = []\n",
        "\n",
        "for cls in class_names:\n",
        "    class_correct = []\n",
        "    test_imgs = glob.glob(dataset.location + '/test/' + cls + '/*.jpg')\n",
        "\n",
        "    for img in test_imgs:\n",
        "        image = transform(Image.open(img)).unsqueeze(0).to(device)\n",
        "\n",
        "        # Use processor to tokenize text\n",
        "        text = [\"a photo of\", \"a picture of\"]\n",
        "        inputs = processor(text, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Directly use the encoded features for classification\n",
        "            outputs = model(**inputs)\n",
        "            probs = torch.nn.functional.softmax(outputs.logits_per_image[0], dim=-1).cpu().numpy()\n",
        "\n",
        "            pred = class_names[probs.argmax()]\n",
        "\n",
        "            if pred == cls:\n",
        "                correct.append(1)\n",
        "                class_correct.append(1)\n",
        "            else:\n",
        "                correct.append(0)\n",
        "                class_correct.append(0)\n",
        "\n",
        "    print(f'Accuracy on class {cls} is: {sum(class_correct)/len(class_correct)}')\n",
        "\n",
        "print(f'Accuracy on all classes is: {sum(correct)/len(correct)}')"
      ],
      "metadata": {
        "id": "4k8218QFFvQV",
        "outputId": "80e058b4-3714-42c8-8bc4-2ec622a62635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on class Cracked Non-Cracked is: 0.0\n",
            "Accuracy on class Spalling is: 1.0\n",
            "Accuracy on class Cracked is: 0.0\n",
            "Accuracy on class Joint Non-Cracked is: 0.0\n",
            "Accuracy on class Non-Cracked is: 0.0\n",
            "Accuracy on all classes is: 0.3225806451612903\n"
          ]
        }
      ]
    }
  ]
}